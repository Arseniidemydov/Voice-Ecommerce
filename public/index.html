<!DOCTYPE html>
<html>
<head>
    <title>Voice Assistant</title>
    <style>
        body {
            background-color: #1a1a1a;
            color: white;
            font-family: Arial, sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            min-height: 100vh;
            margin: 0;
            padding: 20px;
        }

        .voice-button {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            background: #0066ff;
            border: none;
            cursor: pointer;
            position: relative;
            transition: all 0.3s ease;
            margin: 20px;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .voice-button::before {
            content: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" fill="white" class="bi bi-mic" viewBox="0 0 16 16"><path d="M3.5 6.5A.5.5 0 0 1 4 7v1a4 4 0 0 0 8 0V7a.5.5 0 0 1 1 0v1a5 5 0 0 1-4.5 4.975V15h3a.5.5 0 0 1 0 1h-7a.5.5 0 0 1 0-1h3v-2.025A5 5 0 0 1 3 8V7a.5.5 0 0 1 .5-.5"/><path d="M10 8a2 2 0 1 1-4 0V3a2 2 0 1 1 4 0zM8 0a3 3 0 0 0-3 3v5a3 3 0 0 0 6 0V3a3 3 0 0 0-3-3"/></svg>');
            font-size: 40px;
            transition: opacity 0.3s ease;
        }

        .voice-button.loading::before {
            opacity: 0;
        }

        .loading-text {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            text-align: center;
            width: 200px;
            font-size: 16px;
            color: white;
            opacity: 0;
            transition: opacity 0.3s ease;
            display: none;
            font-weight: 500;
        }

        .loading-text.active {
            opacity: 1;
            display: block;
        }

        .voice-button.loading {
            animation: backgroundPulse 2s infinite;
        }

        .voice-button.recording {
            animation: pulse 1.5s infinite;
            background: #ff0000;
        }

        @keyframes backgroundPulse {
            0% { background-color: #0066ff; }
            50% { background-color: #3385ff; }
            100% { background-color: #0066ff; }
        }

        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.05); }
            100% { transform: scale(1); }
        }

        #response {
            margin: 20px;
            padding: 30px 40px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 10px;
            max-width: 600px;
            width: calc(100% - 40px);
            line-height: 1.6;
            font-size: 16px;
        }

        .status-text {
            text-align: center;
            margin-top: 20px;
            font-size: 14px;
            color: #888;
        }

        /* Media styles */
        #response .media-link {
            color: #66b3ff;
            text-decoration: underline;
            display: block;
            margin: 5px 0;
            transition: color 0.3s ease;
        }

        #response .media-link:hover {
            color: #99ccff;
        }

        #response .image-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 15px;
            margin-top: 10px;
        }

        #response .image-container {
            position: relative;
            padding-bottom: 100%;
            overflow: hidden;
            border-radius: 8px;
        }

        #response .image-container img {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: cover;
            cursor: pointer;
            transition: transform 0.3s ease;
        }

        #response .image-container img:hover {
            transform: scale(1.05);
        }
    </style>
</head>
<body>
    <div class="voice-button" id="talkButton"></div>
    <div class="status-text" id="statusText">Click to start recording</div>
    <div id="response"></div>
    
    <script src="https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js"></script>
    <script>
        const ASSISTANT_ID = 'asst_CDvFyzLG7KL7aRuTymxyrY9W';
        const loadingMessages = [
            "I'm thinking now...",
            "Looking for the best option for you...",
            "Almost got it...",
            "Finalizing the perfect response..."
        ];

        let currentMessageIndex = 0;
        let loadingInterval;

        const button = document.querySelector('.voice-button');
        const responseDiv = document.getElementById('response');
        const statusText = document.getElementById('statusText');
        let mediaRecorder;
        let audioChunks = [];

        function startLoadingAnimation() {
            currentMessageIndex = 0;
            const loadingText = document.createElement('div');
            loadingText.className = 'loading-text';
            button.appendChild(loadingText);
            
            function updateLoadingMessage() {
                loadingText.textContent = loadingMessages[currentMessageIndex];
                loadingText.classList.add('active');
                currentMessageIndex = (currentMessageIndex + 1) % loadingMessages.length;
            }

            button.classList.add('loading');
            updateLoadingMessage();
            loadingInterval = setInterval(updateLoadingMessage, 3000);
        }

        function stopLoadingAnimation() {
            button.classList.remove('loading');
            clearInterval(loadingInterval);
            const loadingText = button.querySelector('.loading-text');
            if (loadingText) {
                loadingText.remove();
            }
        }

        function convertContentToHTML(text) {
            let parts = text.split('\n\n');
            let spokenText = parts[0];
            let mediaSection = '';

            function isImageURL(url) {
                return url.match(/\.(jpeg|jpg|gif|png)$/i) != null;
            }

            parts.forEach(part => {
                if (part.startsWith('Links:') || part.startsWith('Images:')) {
                    const urls = part.split('\n').slice(1);
                    
                    if (part.startsWith('Links:')) {
                        mediaSection += '<br><strong>Links:</strong>';
                        urls.forEach(url => {
                            if (url.trim()) {
                                mediaSection += `
                                    <a href="${url.trim()}" target="_blank" class="media-link">
                                        ${url.trim()}
                                    </a>`;
                            }
                        });
                    }
                    
                    if (part.startsWith('Images:')) {
                        mediaSection += '<br><strong>Images:</strong><div class="image-grid">';
                        urls.forEach(url => {
                            if (url.trim()) {
                                mediaSection += `
                                    <div class="image-container">
                                        <img src="${url.trim()}" alt="Product Image" 
                                             onclick="window.open(this.src, '_blank')"
                                             onerror="this.onerror=null; this.src='placeholder.jpg';">
                                    </div>`;
                            }
                        });
                        mediaSection += '</div>';
                    }
                }
            });

            return {
                spokenText: spokenText,
                formattedText: spokenText + mediaSection
            };
        }

        async function sendToOpenAI(blob) {
            try {
                startLoadingAnimation();
                statusText.textContent = 'Processing...';

                const formData = new FormData();
                formData.append('file', blob, 'audio.webm');
                formData.append('model', 'whisper-1');

                const transcriptionResponse = await axios.post('/api/openai/audio/transcriptions', formData);
                const userText = transcriptionResponse.data.text;

                const threadResponse = await axios.post('/api/openai/threads');

                await axios.post(`/api/openai/threads/${threadResponse.data.id}/messages`, {
                    role: 'user',
                    content: userText
                });

                const runResponse = await axios.post(`/api/openai/threads/${threadResponse.data.id}/runs`, {
                    assistant_id: ASSISTANT_ID
                });

                let runStatus;
                do {
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    const statusResponse = await axios.get(
                        `/api/openai/threads/${threadResponse.data.id}/runs/${runResponse.data.id}`
                    );
                    runStatus = statusResponse.data.status;
                } while (runStatus === 'in_progress');

                const messagesResponse = await axios.get(
                    `/api/openai/threads/${threadResponse.data.id}/messages`
                );
                const aiResponse = messagesResponse.data.data[0].content[0].text.value;
                const formattedResponse = convertContentToHTML(aiResponse);

                const speechResponse = await axios.post('/api/openai/audio/speech', {
                    model: "tts-1",
                    input: formattedResponse.spokenText,
                    voice: "alloy"
                }, {
                    responseType: 'arraybuffer'
                });

                const audioBlob = new Blob([speechResponse.data], { type: 'audio/mpeg' });
                const audioUrl = URL.createObjectURL(audioBlob);
                const audio = new Audio(audioUrl);
                await audio.play();

                responseDiv.innerHTML = formattedResponse.formattedText;
                statusText.textContent = 'Click to start recording';

            } catch (error) {
                console.error('Error:', error.response?.data || error);
                responseDiv.textContent = 'Error processing request. Please try again.';
                statusText.textContent = 'Error occurred';
            } finally {
                stopLoadingAnimation();
            }
        }

        button.onclick = async () => {
            try {
                if (!mediaRecorder) {
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    mediaRecorder = new MediaRecorder(stream, {
                        mimeType: 'audio/webm;codecs=opus'
                    });
                    
                    mediaRecorder.ondataavailable = (event) => {
                        audioChunks.push(event.data);
                    };

                    mediaRecorder.onstop = async () => {
                        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                        await sendToOpenAI(audioBlob);
                        button.classList.remove('recording');
                        audioChunks = [];
                    };

                    button.classList.add('recording');
                    statusText.textContent = 'Recording... Click to stop';
                    mediaRecorder.start();
                } else {
                    mediaRecorder.stop();
                    const tracks = mediaRecorder.stream.getTracks();
                    tracks.forEach(track => track.stop());
                    mediaRecorder = null;
                    statusText.textContent = 'Processing...';
                }
            } catch (error) {
                console.error('Microphone error:', error);
                statusText.textContent = 'Error accessing microphone';
            }
        };
    </script>
</body>
</html>